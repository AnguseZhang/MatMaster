# 测评框架流程梳理

本文档结合 MatMaster Agent 代码，详细梳理测评框架的完整工作流程。

## 目录

1. [整体架构](#1-整体架构)
2. [入口与启动流程](#2-入口与启动流程)
3. [Agent 执行流程](#3-agent-执行流程)
4. [测评框架与 Agent 的交互](#4-测评框架与-agent-的交互)
5. [两种测评模式](#5-两种测评模式)
6. [数据流转](#6-数据流转)
7. [关键组件说明](#7-关键组件说明)

---

## 1. 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                    测评框架 (agent-evaluation-main)          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐ │
│  │  main.py     │───▶│  launcher.py │───▶│  runner.py   │ │
│  │  (入口)      │    │  (任务调度)   │    │  (单任务执行) │ │
│  └──────────────┘    └──────────────┘    └──────────────┘ │
│         │                    │                    │         │
│         └────────────────────┴────────────────────┘         │
│                              │                              │
│                    ┌─────────▼─────────┐                   │
│                    │ evaluation.py     │                   │
│                    │ (核心执行逻辑)     │                   │
│                    └─────────┬─────────┘                   │
│                              │                              │
│                    ┌─────────▼─────────┐                   │
│                    │ HumanSimulator    │                   │
│                    │ (用户模拟器)      │                   │
│                    └───────────────────┘                   │
└──────────────────────────────┬──────────────────────────────┘
                               │
                               │ 调用
                               ▼
┌─────────────────────────────────────────────────────────────┐
│              MatMaster Agent (agents/matmaster_agent)        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────────────────────────────────────────────┐  │
│  │         MatMasterFlowAgent (flow_agents/agent.py)    │  │
│  │                                                       │  │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐     │  │
│  │  │intent_agent│  │expand_agent│  │scene_agent │     │  │
│  │  └────────────┘  └────────────┘  └────────────┘     │  │
│  │                                                       │  │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐     │  │
│  │  │plan_make_  │  │plan_confirm│  │execution_  │     │  │
│  │  │agent       │  │_agent      │  │agent       │     │  │
│  │  └────────────┘  └────────────┘  └────────────┘     │  │
│  │                                                       │  │
│  │  ┌────────────┐  ┌────────────┐                     │  │
│  │  │analysis_   │  │report_agent│                     │  │
│  │  │agent       │  └────────────┘                     │  │
│  │  └────────────┘                                     │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

---

## 2. 入口与启动流程

### 2.1 命令行入口

**文件**: `agent-evaluation-main/main.py`

```python
python main.py agent [eval_type]
```

**流程**:
1. 解析命令行参数，确定测评类型（如 `database_search`）
2. 调用 `launcher.py` 的 `main()` 函数
3. 设置环境变量和 Python 路径，确保能导入 `agents.matmaster_agent`

### 2.2 任务调度器 (launcher.py)

**文件**: `agent-evaluation-main/src/agent_evaluator/launcher.py`

**核心流程**:
```python
async def main():
    # 1. 读取测试用例 JSON 文件
    json_path = agent_cases_dir / f"{eval_type}.json"
    dataset = json.load(json_path)
    
    # 2. 创建并发任务（使用 Semaphore 控制并发数）
    tasks = [sem_run_job(i) for i in range(len(dataset))]
    
    # 3. 并发执行所有任务
    await asyncio.gather(*tasks)
```

**关键点**:
- 从 `cases/agent_cases/{eval_type}.json` 读取测试用例
- 每个用例作为一个独立任务，通过 `runner.py` 执行
- 使用 `asyncio.Semaphore` 控制最大并发数（默认 3）
- 每个任务的日志保存到 `{LOG_BASE_DIR}/{eval_type}/logs/item_{item_id}.log`

### 2.3 单任务执行器 (runner.py)

**文件**: `agent-evaluation-main/src/agent_evaluator/base/runner.py`

**流程**:
```python
# runner.py 非常简单，只是调用工具函数
from agent_evaluator.utils import run_single_evaluation

if __name__ == '__main__':
    run_single_evaluation()
```

**实际执行**:
- `utils.py` 的 `run_single_evaluation()` 解析命令行参数
- 调用 `evaluation.py` 的 `evaluation_threads_single_task()`

---

## 3. Agent 执行流程

### 3.1 MatMasterFlowAgent 主流程

**文件**: `agents/matmaster_agent/flow_agents/agent.py`

**核心方法**: `_run_async_impl()`

#### 3.1.1 初始化检查

```python
# 检查配额
if not ctx.session.state['quota_remaining']:
    yield quota_remaining_event
    return
```

#### 3.1.2 文件上传处理

```python
# 处理用户上传的文件
async for handle_upload_event in self.handle_upload_agent.run_async(ctx):
    yield handle_upload_event
```

#### 3.1.3 意图识别

```python
# 识别用户意图：CHAT 或 RESEARCH
if ctx.session.state['intent'].get('type', None) != IntentEnum.RESEARCH:
    async for intent_event in self.intent_agent.run_async(ctx):
        yield intent_event
```

**意图类型**:
- `CHAT`: 简单对话，直接使用 `chat_agent` 处理
- `RESEARCH`: 研究任务，进入完整流程

#### 3.1.4 RESEARCH 模式完整流程

**步骤 1: 问题扩写 (Expand)**

```python
# 检索 ICL 示例
icl_examples = select_examples(user_content, session_id, env, logger)

# 扩写用户问题
self.expand_agent.instruction = EXPAND_INSTRUCTION + EXPAND_INPUT_EXAMPLES_PROMPT
async for expand_event in self.expand_agent.run_async(ctx):
    yield expand_event
```

**步骤 2: 场景分类 (Scene)**

```python
# 根据扩写后的问题，识别场景标签
self.scene_agent.instruction = SCENE_INSTRUCTION + UPDATE_USER_CONTENT + SCENE_EXAMPLES_PROMPT
async for scene_event in self.scene_agent.run_async(ctx):
    yield scene_event

# 更新场景列表（包含 'universal'）
scenes = list(set(before_scenes + single_scene + ['universal']))
```

**步骤 3: 计划确认检查**

```python
# 检查是否需要用户确认计划
if check_plan(ctx) == FlowStatusEnum.COMPLETE or not plan_confirm:
    async for plan_confirm_event in self.plan_confirm_agent.run_async(ctx):
        yield plan_confirm_event
```

**步骤 4: 制定计划 (Plan Make)**

```python
# 判断是否需要制定新计划
if (check_plan(ctx) in [NO_PLAN, COMPLETE, FAILED] or not plan_confirm):
    # 获取可用工具列表（根据场景过滤）
    available_tools = get_tools_list(ctx, scenes)
    
    # 制定计划
    self.plan_make_agent.instruction = get_plan_make_instruction(...)
    self.plan_make_agent.output_schema = create_dynamic_multi_plans_schema(available_tools)
    async for plan_event in self.plan_make_agent.run_async(ctx):
        yield plan_event
    
    # 总结计划（plan_info_agent）
    async for plan_summary_event in self.plan_info_agent.run_async(ctx):
        yield plan_summary_event
```

**步骤 5: 执行计划 (Execution)**

```python
# 如果计划已确认
if ctx.session.state['plan_confirm']['flag']:
    # 重置 scenes
    yield update_state_event(ctx, state_delta={'scenes': []})
    
    # 执行计划（如果可行性为 full 或 part）
    if ctx.session.state['plan']['feasibility'] in ['full', 'part']:
        async for execution_event in self.execution_agent.run_async(ctx):
            yield execution_event
```

**execution_agent** 是一个 Supervisor Agent，包含多个子 Agent:
- 各种工具对应的 Agent（如 `database_search_agent`, `calculation_agent` 等）
- `step_title_agent`: 为每步生成标题
- `step_validation_agent`: 校验步骤执行结果

**步骤 6: 总结与报告 (Analysis & Report)**

```python
# 如果计划执行完成
if check_plan(ctx) == FlowStatusEnum.COMPLETE:
    # 生成执行总结
    async for analysis_event in self.analysis_agent.run_async(ctx):
        yield analysis_event
    
    # 生成 Markdown 报告
    async for report_event in self.report_agent.run_async(ctx):
        yield report_event
    
    # 上传报告到 OSS
    upload_result = await upload_report_md_to_oss(...)
```

#### 3.1.5 循环与重试

```python
# 最多循环 2 次（用于重试失败的计划）
for _ in range(2):
    # ... 执行流程 ...
    
    if check_plan(ctx) == FlowStatusEnum.FAILED:
        loop_continue = True  # 触发重试
        continue
    
    if not loop_continue:
        break
```

---

## 4. 测评框架与 Agent 的交互

### 4.1 核心交互流程

**文件**: `agent-evaluation-main/src/agent_evaluator/base/evaluation.py`

#### 4.1.1 初始化阶段

```python
async def _run_conversation(dataset_item, max_turn_count, item_id, ...):
    # 1. 创建 ADK Session 和 Runner
    session_service = InMemorySessionService()
    artifact_service = InMemoryArtifactService()
    session = await session_service.create_session(
        app_name='matmaster_agent',
        user_id='human_simulator_test',
    )
    
    runner = Runner(
        app_name='matmaster_agent',
        agent=root_agent,  # 这是 MatMasterFlowAgent 的实例
        session_service=session_service,
        artifact_service=artifact_service,
    )
    
    # 2. 创建人类模拟器
    simulator = HumanSimulator(max_turn_count=max_turn_count)
    
    # 3. 设置对话目标
    scenario = {
        'name': dataset_item['initial_question'],
        'goal': ConversationGoal(
            initial_question=dataset_item['initial_question'],
            expected_outcomes=dataset_item['expected_outcomes'],
            success_criteria=dataset_item['success_criteria'],
        ),
    }
    simulator.set_goal(scenario['goal'])
```

#### 4.1.2 对话循环

```python
turn_count = 0
while turn_count < max_turn_count:
    turn_count += 1
    
    # 1. 获取用户输入
    user_input = (
        initial_question if turn_count == 1 
        else simulator.get_last_user_response()
    )
    
    # 2. 调用 Agent（关键步骤）
    content = types.Content(
        role='user', 
        parts=[types.Part(text=user_input)]
    )
    
    events = runner.run_async(
        user_id=session.user_id,
        session_id=session.id,
        new_message=content,
        run_config=RunConfig(streaming_mode=StreamingMode.SSE),
    )
    
    # 3. 收集 Agent 事件流
    events_list = []
    agent_response = ''
    async for event in events:
        # 提取文本内容
        if event.content and event.content.parts:
            for part in event.content.parts:
                if part.text:
                    agent_response += part.text
        
        # 保存事件（用于后续分析）
        events_list.append(event.dict() if hasattr(event, 'dict') else dict(event))
    
    # 4. 保存事件到文件
    with open(f"{label_key}/logs/job_{item_id}/turn_{turn_count}.txt", 'w') as f:
        f.write(str(events_list))
    
    # 5. 生成模拟用户响应
    user_response, should_continue = simulator.generate_response(agent_response)
    
    # 6. 检查是否继续
    if not should_continue:
        break
```

#### 4.1.3 事件捕获（截断模式）

在 **工具调用验证模式** (`tool_call_only`) 下，测评框架会捕获 `multi_plans` 状态：

```python
# 检测截断模式
evaluation_mode = dataset_item.get('evaluation_mode', 'full_execution')
is_tool_call_only_mode = evaluation_mode == 'tool_call_only'

if is_tool_call_only_mode:
    async for event in events:
        # 捕获 state_delta 中的 multi_plans
        if hasattr(event, 'actions') and event.actions:
            state_delta = event.actions.state_delta
            if hasattr(state_delta, 'multi_plans') and state_delta.multi_plans:
                captured_multi_plans = state_delta.multi_plans.dict()
                
                # 提取工具调用并验证
                actual_tool_calls = extract_tool_calls_from_plan(captured_multi_plans)
                is_valid, reason, details = validate_tool_calls(
                    actual_tool_calls, 
                    expected_tool_calls
                )
                
                # 立即返回验证结果
                return eval_results
```

---

## 5. 两种测评模式

### 5.1 完整执行模式 (full_execution)

**特点**:
- 执行完整的 Agent 流程，包括计划制定、执行、总结
- 支持多轮对话
- 使用 `HumanSimulator` 模拟用户响应
- 最终验证结果是否符合 `success_criteria`

**流程**:
```
用户输入 → Agent 处理 → 获取响应 → HumanSimulator 生成回复 → 
下一轮输入 → ... → 达到成功标准或最大轮次
```

**验证方式**:
- 通过 `HumanSimulator` 的 LLM 判断任务是否完成
- 检查最终结果是否满足 `success_criteria`

### 5.2 工具调用验证模式 (tool_call_only)

**特点**:
- 仅验证 Agent 生成的计划中的工具调用是否正确
- 在第一轮对话后立即截断，不执行实际工具
- 快速验证，成本低

**流程**:
```
用户输入 → Agent 处理 → 捕获 multi_plans → 
提取工具调用 → 验证工具名称和描述 → 返回验证结果
```

**验证逻辑** (`validate_tool_calls`):
1. 验证工具调用数量是否足够
2. 逐个验证工具名称是否匹配（支持 `alternative_tools`）
3. 验证描述中是否包含必需关键词 (`description_must_contain`)

**测试用例格式**:
```json
{
  "initial_question": "...",
  "evaluation_mode": "tool_call_only",
  "expected_tool_calls": [
    {
      "tool_name": "fetch_structures_with_bandgap",
      "alternative_tools": ["fetch_structures_with_filter"],
      "description_must_contain": ["带隙", "氧化物"]
    }
  ]
}
```

---

## 6. 数据流转

### 6.1 输入数据

**测试用例 JSON** (`cases/agent_cases/{eval_type}.json`):
```json
{
  "initial_question": "在 Materials Project 中检索并返回3个带隙大于 2 eV 的氧化物结构",
  "expected_outcomes": ["找到 Materials Project 中带隙 >2 eV 的氧化物"],
  "success_criteria": [
    "所有结果均为氧化物",
    "返回文件URL",
    "结果数量正确"
  ],
  "evaluation_mode": "full_execution",  // 或 "tool_call_only"
  "expected_tool_calls": [...]  // 仅 tool_call_only 模式需要
}
```

### 6.2 Agent 状态流转

**关键状态字段** (`ctx.session.state`):
- `intent`: 用户意图（CHAT/RESEARCH）
- `expand`: 扩写后的问题
- `scenes`: 场景标签列表
- `multi_plans`: 生成的多个计划
- `plan_confirm`: 计划确认状态
- `plan`: 当前选中的计划
- `plan_info`: 计划总结信息

**状态更新事件**:
```python
# Agent 通过 state_delta 更新状态
yield update_state_event(ctx, state_delta={'intent': {...}})
```

### 6.3 输出数据

**事件日志** (`{label_key}/logs/job_{item_id}/turn_{turn_count}.txt`):
- 保存每轮对话的所有事件
- 包含文本响应、函数调用、状态更新等

**测评结果** (`evaluation_results.json`):
```json
{
  "initial_question": "...",
  "expected_outcomes": [...],
  "success_criteria": [...],
  "agent_response_1": "...",
  "user_response_1": "...",
  "total_turns": 3,
  "final_state": "satisfied",
  "duration_minutes": 2.5
}
```

**工具调用验证结果** (tool_call_only 模式):
```json
{
  "evaluation_mode": "tool_call_only",
  "status": "PASSED",
  "tool_calls": [...],
  "expected_tool_calls": [...],
  "validation_result": true,
  "validation_reason": "工具调用验证通过",
  "validation_details": {...}
}
```

---

## 7. 关键组件说明

### 7.1 HumanSimulator

**文件**: `agent-evaluation-main/src/agent_evaluator/base/human_simulator.py`

**职责**:
- 模拟真实用户行为
- 基于 Agent 回复生成上下文相关的响应
- 判断任务是否完成

**核心方法**:
- `generate_response(agent_message)`: 使用 LLM 生成用户响应
- `get_bohr_results(agent_message, job_ids)`: 处理异步任务结果查询
- `get_conversation_summary()`: 生成对话摘要

**响应生成规则**:
- 如果 Agent 只给出计划，要求继续执行
- 如果 Agent 给出最终结果，判断是否满足标准
- 如果结果完整且满足目标，结束对话

### 7.2 Runner (ADK)

**职责**:
- 管理 Agent 的会话生命周期
- 处理用户输入，调用 Agent
- 收集 Agent 事件流

**关键配置**:
- `streaming_mode=StreamingMode.SSE`: 流式返回事件
- `session_service`: 内存会话服务
- `artifact_service`: 内存文件服务

### 7.3 工具调用提取与验证

**函数**: `extract_tool_calls_from_plan(multi_plans)`

**流程**:
1. 从 `multi_plans.plans[0].steps` 提取工具调用
2. 过滤掉没有 `tool_name` 的步骤
3. 返回工具调用列表，包含 `tool_name`, `args`, `description`

**函数**: `validate_tool_calls(actual_calls, expected_calls)`

**验证项**:
1. 工具调用数量
2. 工具名称匹配（支持替代工具）
3. 描述关键词匹配

---

## 8. 总结

### 8.1 完整流程概览

```
1. 启动测评
   main.py → launcher.py → runner.py

2. 执行单个测试用例
   evaluation_threads_single_task() → _run_conversation()

3. 初始化环境
   创建 Session → 创建 Runner → 创建 HumanSimulator

4. 对话循环
   for turn in range(max_turn_count):
       a. 获取用户输入（首轮用 initial_question，后续用 HumanSimulator）
       b. 调用 Agent (runner.run_async)
       c. Agent 内部流程：
          - 意图识别 → CHAT/RESEARCH
          - RESEARCH 模式：
            * 问题扩写
            * 场景分类
            * 计划确认检查
            * 制定计划（如需要）
            * 执行计划（如已确认）
            * 总结与报告（如完成）
       d. 收集 Agent 事件流
       e. 保存事件到文件
       f. HumanSimulator 生成用户响应
       g. 判断是否继续

5. 结果保存
   保存到 evaluation_results.json
```

### 8.2 关键设计点

1. **异步并发**: 使用 `asyncio` 实现任务并发执行
2. **事件驱动**: Agent 通过事件流 (`AsyncGenerator[Event]`) 返回结果
3. **状态管理**: 使用 ADK Session 管理对话状态
4. **可扩展性**: 支持多种测评模式（完整执行/工具调用验证）
5. **可追溯性**: 保存完整事件日志，支持回放和分析

### 8.3 与 Agent 的集成点

1. **Agent 实例**: `root_agent` (MatMasterFlowAgent)
2. **事件捕获**: 从 `runner.run_async()` 的事件流中提取信息
3. **状态访问**: 通过 `event.actions.state_delta` 访问 Agent 状态
4. **工具调用验证**: 从 `multi_plans` 状态中提取工具调用信息

---

## 附录：相关文件路径

- **测评框架入口**: `agent-evaluation-main/main.py`
- **任务调度**: `agent-evaluation-main/src/agent_evaluator/launcher.py`
- **单任务执行**: `agent-evaluation-main/src/agent_evaluator/base/runner.py`
- **核心执行逻辑**: `agent-evaluation-main/src/agent_evaluator/base/evaluation.py`
- **人类模拟器**: `agent-evaluation-main/src/agent_evaluator/base/human_simulator.py`
- **Agent 主流程**: `agents/matmaster_agent/flow_agents/agent.py`
- **测试用例**: `agent-evaluation-main/cases/agent_cases/{eval_type}.json`
- **日志输出**: `agent-evaluation-main/{label_key}/logs/job_{item_id}/turn_{turn_count}.txt`
